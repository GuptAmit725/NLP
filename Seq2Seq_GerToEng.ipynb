{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_GerToEng.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHxBDPUeUBn"
      },
      "source": [
        "<h1><I>Installling necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgQi9g3TIxPB"
      },
      "source": [
        "!pip install torchtext==0.4\n",
        "!pip install -U spacy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_2Fn9V3Jf8d"
      },
      "source": [
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gald9ifzeZqB"
      },
      "source": [
        "<h1><I>Importing necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KUg8nocbgZFL",
        "outputId": "7d7c0a5d-71a1-45db-955c-b8c61149bebd"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import spacy as sp\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from torch.utils.tensorboard import SummaryWriter "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZNOzjUHehJd"
      },
      "source": [
        "<h1><I>Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37_HUqVE8wkw"
      },
      "source": [
        "import spacy\n",
        "spacy_eng = sp.load(\"en_core_web_sm\")\n",
        "spacy_ger = spacy.load(\"de_core_news_sm\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZYrAzyaeqPu"
      },
      "source": [
        "<h1><I>Tokenizing the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQvDNk63HJdH"
      },
      "source": [
        "def tokenize_ger(text):\n",
        "  return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "\n",
        "def tokenize_eng(text):\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CNA0CYxL0I7"
      },
      "source": [
        "german = Field(tokenize=tokenize_ger, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "english = Field(tokenize=tokenize_eng, lower=True, init_token='<sos>', eos_token='<eos>')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0_I-Evbew6n"
      },
      "source": [
        "<h1><I>Splitting the data into train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyZ9PTdiMWKz"
      },
      "source": [
        "train_data, validation_data, test_data = Multi30k.splits( exts=('.de', '.en'), fields=(german,english))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WMxro0_qM0C0",
        "outputId": "5582a886-ca48-4bd0-f139-407f6528c670"
      },
      "source": [
        "type(spacy_eng)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX0oc_GOe4GC"
      },
      "source": [
        "<h1><I>Building the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymwRBDjxM9Og"
      },
      "source": [
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmARRAPEe9qS"
      },
      "source": [
        "<h1><I>CReating the classes of Encoder and decoder and creating model out of it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6yjeRqkfIAb"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, \n",
        "               hidden_size,num_layers,d_rate):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.dropout = nn.Dropout(d_rate)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=d_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('ENCODER INPUT SHAPE ======= ',x.shape) # (N, seq_length)\n",
        "    x = self.embedding(x)\n",
        "    #print('EMBEDDING SHAPE ======= ',x.shape)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    output, (hidden,cell) = self.lstm(x)\n",
        "    #print('ENCODER OUTPUT SHAPE ======= ',output.shape)\n",
        "\n",
        "    #print('=======================END OF ENCODER===================')\n",
        "    return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_size, \n",
        "               embedding_size, \n",
        "               hidden_size, \n",
        "               output_size, \n",
        "               num_layers, d_rate\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.dropout = nn.Dropout(d_rate)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size,num_layers, dropout=d_rate)\n",
        "    self.fc = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "  def forward(self,x, hidden, cell):\n",
        "    #print('DECODER INPUT SHAPE ====== ', x.shape)\n",
        "    x = x.unsqueeze(0)\n",
        "    x = self.embedding(x)\n",
        "    #x = x.reshape(x.shape[1], x.shape[2], x.shape[3])\n",
        "    #print('DECODER EMBEDDING SHAPE ====== ', x.shape)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    output, (hidden, cell) = self.lstm(x)\n",
        "    #print('DECODER OUTPUT SHAPE ====== ', output.shape)\n",
        "    pred = self.fc(output).squeeze(0)\n",
        "    #print('DECODER PREDICTION SHAPE ====== ', pred.shape)\n",
        "\n",
        "    #print('========================END OF DECODER========================')\n",
        "\n",
        "    return pred, hidden, cell\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, src, trg, teacher_force_ratio=0.5):\n",
        "    batch_size = src.shape[1]\n",
        "    target_len = trg.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "    # Since we are doing word by word prediction\n",
        "\n",
        "    hidden, cell = self.encoder(src)\n",
        "\n",
        "    #grabing start token\n",
        "    x = trg[0]\n",
        "\n",
        "    for t in range(1,target_len):\n",
        "      output, hidden, cell = self.decoder(x,hidden, cell)\n",
        "      outputs[t] = output\n",
        "      best_guess = output.argmax(1)\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "      # In this line we are feeding actual target value and output of the previous decoder alternatively.\n",
        "    return outputs\n",
        "\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvalF279fIry"
      },
      "source": [
        "<h1><I>Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4BEcgCEffmI6",
        "outputId": "6ad10dd4-e850-4c03-8ce2-50e54619e561"
      },
      "source": [
        "#training hyper parameters\n",
        "epochs = 20\n",
        "lr = 0.0001\n",
        "batch_size = 64\n",
        "\n",
        "#Model hyperparameters\n",
        "load_model = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size_enc = len(german.vocab)\n",
        "input_size_dec = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "enc_droput = 0.5\n",
        "dec_dropout = 0.55\n",
        "optimizer = torch.optim.Adam(model_.parameters(),lr = lr)\n",
        "#tensorboard\n",
        "\n",
        "writer = SummaryWriter(f'runs/loss_plot')\n",
        "step = 0\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x: len(x.src),\n",
        "    device = device\n",
        ")\n",
        "\n",
        "enc_net = Encoder(input_size_enc,\n",
        "                  encoder_embedding_size, \n",
        "                  hidden_size, num_layers,\n",
        "                  dec_dropout).to(device)\n",
        "\n",
        "dec_net = Decoder(input_size_dec, \n",
        "                  decoder_embedding_size,\n",
        "                  hidden_size, output_size,\n",
        "                  num_layers, dec_dropout\n",
        "                  ).to(device)\n",
        "\n",
        "\n",
        "model_ = model(enc_net, dec_net).to(device)\n",
        "\n",
        "pad_idx = english.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "if load_model:\n",
        "  load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model_, optimizer)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f'Epoch [{epoch} / {epochs}]')\n",
        "  #checkpoint = {'state_dict':model_.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "  #save_checkpoint(checkoint) \n",
        "\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    inp_data = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "    #print('INP SHAPE ===== ', inp_data.shape, target.shape)\n",
        "    output = model_(inp_data, target)\n",
        "\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model_.parameters(), max_norm = 1)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    writer.add_scalar('Training Loss', loss, global_step=step)\n",
        "    step += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0 / 20]\n",
            "Epoch [1 / 20]\n",
            "Epoch [2 / 20]\n",
            "Epoch [3 / 20]\n",
            "Epoch [4 / 20]\n",
            "Epoch [5 / 20]\n",
            "Epoch [6 / 20]\n",
            "Epoch [7 / 20]\n",
            "Epoch [8 / 20]\n",
            "Epoch [9 / 20]\n",
            "Epoch [10 / 20]\n",
            "Epoch [11 / 20]\n",
            "Epoch [12 / 20]\n",
            "Epoch [13 / 20]\n",
            "Epoch [14 / 20]\n",
            "Epoch [15 / 20]\n",
            "Epoch [16 / 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0uaZF4geZ4l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}